{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 10000\n"
     ]
    }
   ],
   "source": [
    "train_len = len(trainset)\n",
    "test_len = len(testset)\n",
    "index = list(range(train_len))\n",
    "print(train_len, test_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct validation set (lets use 10 percent)\n",
    "np.random.shuffle(index)\n",
    "#number of blocks of data\n",
    "split = int(0.5 * train_len)\n",
    "set1_index = index[split:]\n",
    "set2_index = index[:split]\n",
    "#Need to use a dataloader to control batch size and also enable SGD\n",
    "set1_loader = torch.utils.data.DataLoader(trainset, sampler = set1_index, batch_size = 12, num_workers = 10)\n",
    "set2_loader = torch.utils.data.DataLoader(trainset, sampler = set2_index, batch_size = 12, num_workers = 10)\n",
    "test_loader = torch.utils.data.DataLoader(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "#dataset only for testing don't run this piece of code if it's not necessary\n",
    "split = int(0.01*train_len)\n",
    "set1_index = index[0:split]\n",
    "set2_index = index[split:2*split]\n",
    "print(len(set1_index))\n",
    "print(len(set2_index))\n",
    "set1_loader = torch.utils.data.DataLoader(trainset, sampler = set1_index, batch_size = 12, num_workers = 10)\n",
    "set2_loader = torch.utils.data.DataLoader(trainset, sampler = set2_index, batch_size = 12, num_workers = 10)\n",
    "test_loader = torch.utils.data.DataLoader(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "set1dataiter = iter(set1_loader)\n",
    "set1images, set1labels = set1dataiter.next()\n",
    "set2dataiter = iter(set2_loader)\n",
    "set2images, set2labels = set2dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNBlock1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size = 3, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size = 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x),2))\n",
    "        return x\n",
    "cnnblock1 = CNNBlock1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNBlock2, self).__init__()\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size = 3, padding = 1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size = 4, padding = 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(F.max_pool2d(self.conv4(x),2))\n",
    "        return x\n",
    "cnnblock2 = CNNBlock2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNBlock3, self).__init__()\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size = 3, padding = 1)\n",
    "        self.conv6 = nn.Conv2d(256, 512, kernel_size = 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        return x\n",
    "cnnblock3 = CNNBlock3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPBlock1, self).__init__()\n",
    "        self.fc1 = nn.Linear(15*15*128, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 15*15*128)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "mlpblock1 = MLPBlock1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPBlock2, self).__init__()\n",
    "        self.fc4 = nn.Linear(7*7*256, 256)\n",
    "        self.fc5 = nn.Linear(256, 64)\n",
    "        self.fc6 = nn.Linear(64,10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 7*7*256)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "mlpblock2 = MLPBlock2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPBlock3, self).__init__()\n",
    "        self.fc7 = nn.Linear(6*6*512, 256)\n",
    "        self.fc8 = nn.Linear(256, 64)\n",
    "        self.fc9 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 6*6*512)\n",
    "        x = F.relu(self.fc7(x))\n",
    "        x = F.relu(self.fc8(x))\n",
    "        x = self.fc9(x)\n",
    "        return x\n",
    "mlpblock3 = MLPBlock3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.cnnblock1 = cnnblock1\n",
    "        self.cnnblock2 = cnnblock2\n",
    "        self.cnnblock3 = cnnblock3\n",
    "        self.mlpblock1 = mlpblock1\n",
    "        self.mlpblock2 = mlpblock2\n",
    "        self.mlpblock3 = mlpblock3\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.cnnblock1(x)\n",
    "        x2 = self.cnnblock2(x1)\n",
    "        x3 = self.cnnblock3(x2)\n",
    "        x4 = self.mlpblock1(x1)\n",
    "        x5 = self.mlpblock2(x2)\n",
    "        x6 = self.mlpblock3(x3)\n",
    "        x7 = x4 + x5 + x6\n",
    "        return F.log_softmax(x7, dim=1)\n",
    "\n",
    "ensemblemodel = EnsembleModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(ensemblemodel.parameters(), lr = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   20] loss: 2.313\n",
      "[1,   40] loss: 2.299\n",
      "[2,   21] loss: 2.277\n",
      "[2,   42] loss: 2.277\n",
      "[3,   20] loss: 2.254\n",
      "[3,   40] loss: 2.222\n",
      "[4,   21] loss: 2.164\n",
      "[4,   42] loss: 2.176\n",
      "[5,   20] loss: 2.091\n",
      "[5,   40] loss: 2.098\n",
      "[6,   21] loss: 2.086\n",
      "[6,   42] loss: 2.118\n",
      "[7,   20] loss: 2.016\n",
      "[7,   40] loss: 2.047\n",
      "[8,   21] loss: 2.083\n",
      "[8,   42] loss: 2.107\n",
      "[9,   20] loss: 1.929\n",
      "[9,   40] loss: 1.988\n",
      "[10,   21] loss: 2.087\n",
      "[10,   42] loss: 2.085\n",
      "[11,   20] loss: 1.849\n",
      "[11,   40] loss: 1.937\n",
      "[12,   21] loss: 2.079\n",
      "[12,   42] loss: 2.063\n",
      "[13,   20] loss: 1.811\n",
      "[13,   40] loss: 1.890\n",
      "[14,   21] loss: 2.088\n",
      "[14,   42] loss: 2.075\n",
      "[15,   20] loss: 1.700\n",
      "[15,   40] loss: 1.778\n",
      "[16,   21] loss: 2.070\n",
      "[16,   42] loss: 2.056\n",
      "[17,   20] loss: 1.665\n",
      "[17,   40] loss: 1.717\n",
      "[18,   21] loss: 2.068\n",
      "[18,   42] loss: 2.072\n",
      "[19,   20] loss: 1.587\n",
      "[19,   40] loss: 1.596\n",
      "[20,   21] loss: 1.999\n",
      "[20,   42] loss: 2.031\n",
      "[21,   20] loss: 1.535\n",
      "[21,   40] loss: 1.560\n",
      "[22,   21] loss: 1.947\n",
      "[22,   42] loss: 2.018\n",
      "[23,   20] loss: 1.467\n",
      "[23,   40] loss: 1.466\n",
      "[24,   21] loss: 1.892\n",
      "[24,   42] loss: 2.005\n",
      "[25,   20] loss: 1.433\n",
      "[25,   40] loss: 1.388\n",
      "[26,   21] loss: 1.811\n",
      "[26,   42] loss: 1.983\n",
      "[27,   20] loss: 1.384\n",
      "[27,   40] loss: 1.333\n",
      "[28,   21] loss: 1.794\n",
      "[28,   42] loss: 1.961\n",
      "[29,   20] loss: 1.361\n",
      "[29,   40] loss: 1.249\n",
      "[30,   21] loss: 1.711\n",
      "[30,   42] loss: 1.886\n",
      "[31,   20] loss: 1.429\n",
      "[31,   40] loss: 1.333\n",
      "[32,   21] loss: 1.760\n",
      "[32,   42] loss: 1.849\n",
      "[33,   20] loss: 1.254\n",
      "[33,   40] loss: 1.152\n",
      "[34,   21] loss: 1.633\n",
      "[34,   42] loss: 1.795\n",
      "[35,   20] loss: 1.162\n",
      "[35,   40] loss: 1.082\n",
      "[36,   21] loss: 1.635\n",
      "[36,   42] loss: 1.782\n",
      "[37,   20] loss: 1.144\n",
      "[37,   40] loss: 1.075\n",
      "[38,   21] loss: 1.596\n",
      "[38,   42] loss: 1.749\n",
      "[39,   20] loss: 1.045\n",
      "[39,   40] loss: 0.982\n",
      "[40,   21] loss: 1.559\n",
      "[40,   42] loss: 1.644\n",
      "[41,   20] loss: 1.014\n",
      "[41,   40] loss: 0.910\n",
      "[42,   21] loss: 1.800\n",
      "[42,   42] loss: 1.737\n",
      "[43,   20] loss: 1.054\n",
      "[43,   40] loss: 0.975\n",
      "[44,   21] loss: 1.608\n",
      "[44,   42] loss: 1.674\n",
      "[45,   20] loss: 1.039\n",
      "[45,   40] loss: 1.034\n",
      "[46,   21] loss: 1.681\n",
      "[46,   42] loss: 1.819\n",
      "[47,   20] loss: 0.918\n",
      "[47,   40] loss: 0.904\n",
      "[48,   21] loss: 1.551\n",
      "[48,   42] loss: 1.672\n",
      "[49,   20] loss: 0.882\n",
      "[49,   40] loss: 0.770\n",
      "[50,   21] loss: 1.522\n",
      "[50,   42] loss: 1.646\n",
      "[51,   20] loss: 0.742\n",
      "[51,   40] loss: 0.657\n",
      "[52,   21] loss: 1.422\n",
      "[52,   42] loss: 1.546\n",
      "[53,   20] loss: 0.761\n",
      "[53,   40] loss: 0.672\n",
      "[54,   21] loss: 1.576\n",
      "[54,   42] loss: 1.661\n",
      "[55,   20] loss: 0.770\n",
      "[55,   40] loss: 0.811\n",
      "[56,   21] loss: 1.837\n",
      "[56,   42] loss: 1.630\n",
      "[57,   20] loss: 0.648\n",
      "[57,   40] loss: 0.560\n",
      "[58,   21] loss: 1.430\n",
      "[58,   42] loss: 1.647\n",
      "[59,   20] loss: 0.621\n",
      "[59,   40] loss: 0.559\n",
      "[60,   21] loss: 1.397\n",
      "[60,   42] loss: 1.546\n",
      "[61,   20] loss: 0.476\n",
      "[61,   40] loss: 0.467\n",
      "[62,   21] loss: 1.317\n",
      "[62,   42] loss: 1.581\n",
      "[63,   20] loss: 0.485\n",
      "[63,   40] loss: 0.457\n",
      "[64,   21] loss: 1.305\n",
      "[64,   42] loss: 1.433\n",
      "[65,   20] loss: 0.440\n",
      "[65,   40] loss: 0.560\n",
      "[66,   21] loss: 1.299\n",
      "[66,   42] loss: 1.520\n",
      "[67,   20] loss: 0.477\n",
      "[67,   40] loss: 0.420\n",
      "[68,   21] loss: 1.222\n",
      "[68,   42] loss: 1.365\n",
      "[69,   20] loss: 0.505\n",
      "[69,   40] loss: 0.412\n",
      "[70,   21] loss: 1.410\n",
      "[70,   42] loss: 1.345\n",
      "[71,   20] loss: 0.337\n",
      "[71,   40] loss: 0.330\n",
      "[72,   21] loss: 1.227\n",
      "[72,   42] loss: 1.398\n",
      "[73,   20] loss: 0.227\n",
      "[73,   40] loss: 0.217\n",
      "[74,   21] loss: 1.185\n",
      "[74,   42] loss: 1.611\n",
      "[75,   20] loss: 0.592\n",
      "[75,   40] loss: 0.661\n",
      "[76,   21] loss: 1.390\n",
      "[76,   42] loss: 1.458\n",
      "[77,   20] loss: 0.440\n",
      "[77,   40] loss: 0.485\n",
      "[78,   21] loss: 2.452\n",
      "[78,   42] loss: 1.887\n",
      "[79,   20] loss: 0.980\n",
      "[79,   40] loss: 1.116\n",
      "[80,   21] loss: 1.651\n",
      "[80,   42] loss: 1.769\n",
      "[81,   20] loss: 1.018\n",
      "[81,   40] loss: 1.005\n",
      "[82,   21] loss: 1.489\n",
      "[82,   42] loss: 1.652\n",
      "[83,   20] loss: 0.564\n",
      "[83,   40] loss: 0.548\n",
      "[84,   21] loss: 1.415\n",
      "[84,   42] loss: 1.550\n",
      "[85,   20] loss: 0.531\n",
      "[85,   40] loss: 0.559\n",
      "[86,   21] loss: 1.390\n",
      "[86,   42] loss: 1.466\n",
      "[87,   20] loss: 0.406\n",
      "[87,   40] loss: 0.400\n",
      "[88,   21] loss: 1.508\n",
      "[88,   42] loss: 1.632\n",
      "[89,   20] loss: 0.957\n",
      "[89,   40] loss: 0.805\n",
      "[90,   21] loss: 1.525\n",
      "[90,   42] loss: 1.433\n",
      "[91,   20] loss: 0.343\n",
      "[91,   40] loss: 0.288\n",
      "[92,   21] loss: 1.292\n",
      "[92,   42] loss: 1.340\n",
      "[93,   20] loss: 0.246\n",
      "[93,   40] loss: 0.268\n",
      "[94,   21] loss: 1.170\n",
      "[94,   42] loss: 1.306\n",
      "[95,   20] loss: 0.249\n",
      "[95,   40] loss: 0.268\n",
      "[96,   21] loss: 1.231\n",
      "[96,   42] loss: 1.105\n",
      "[97,   20] loss: 0.163\n",
      "[97,   40] loss: 0.161\n",
      "[98,   21] loss: 1.289\n",
      "[98,   42] loss: 1.323\n",
      "[99,   20] loss: 0.224\n",
      "[99,   40] loss: 0.187\n",
      "[100,   21] loss: 1.343\n",
      "[100,   42] loss: 1.149\n",
      "[101,   20] loss: 0.164\n",
      "[101,   40] loss: 0.174\n",
      "[102,   21] loss: 1.096\n",
      "[102,   42] loss: 0.894\n",
      "[103,   20] loss: 0.078\n",
      "[103,   40] loss: 0.077\n",
      "[104,   21] loss: 0.932\n",
      "[104,   42] loss: 0.823\n",
      "[105,   20] loss: 0.074\n",
      "[105,   40] loss: 0.070\n",
      "[106,   21] loss: 0.815\n",
      "[106,   42] loss: 0.770\n",
      "[107,   20] loss: 0.073\n",
      "[107,   40] loss: 0.052\n",
      "[108,   21] loss: 0.740\n",
      "[108,   42] loss: 0.752\n",
      "[109,   20] loss: 0.061\n",
      "[109,   40] loss: 0.060\n",
      "[110,   21] loss: 0.682\n",
      "[110,   42] loss: 0.639\n",
      "[111,   20] loss: 0.086\n",
      "[111,   40] loss: 0.066\n",
      "[112,   21] loss: 0.726\n",
      "[112,   42] loss: 0.632\n",
      "[113,   20] loss: 0.022\n",
      "[113,   40] loss: 0.012\n",
      "[114,   21] loss: 0.603\n",
      "[114,   42] loss: 0.709\n",
      "[115,   20] loss: 0.051\n",
      "[115,   40] loss: 0.052\n",
      "[116,   21] loss: 0.831\n",
      "[116,   42] loss: 0.935\n",
      "[117,   20] loss: 0.075\n",
      "[117,   40] loss: 0.091\n",
      "[118,   21] loss: 0.721\n",
      "[118,   42] loss: 0.677\n",
      "[119,   20] loss: 0.031\n",
      "[119,   40] loss: 0.015\n",
      "[120,   21] loss: 0.534\n",
      "[120,   42] loss: 0.579\n",
      "[121,   20] loss: 0.013\n",
      "[121,   40] loss: 0.013\n",
      "[122,   21] loss: 0.477\n",
      "[122,   42] loss: 0.453\n",
      "[123,   20] loss: 0.006\n",
      "[123,   40] loss: 0.003\n",
      "[124,   21] loss: 0.354\n",
      "[124,   42] loss: 0.447\n",
      "[125,   20] loss: 0.016\n",
      "[125,   40] loss: 0.012\n",
      "[126,   21] loss: 0.706\n",
      "[126,   42] loss: 0.554\n",
      "[127,   20] loss: 0.022\n",
      "[127,   40] loss: 0.015\n",
      "[128,   21] loss: 0.532\n",
      "[128,   42] loss: 0.409\n",
      "[129,   20] loss: 0.010\n",
      "[129,   40] loss: 0.016\n",
      "[130,   21] loss: 0.455\n",
      "[130,   42] loss: 1.600\n",
      "[131,   20] loss: 3.428\n",
      "[131,   40] loss: 1.860\n",
      "[132,   21] loss: 2.156\n",
      "[132,   42] loss: 1.603\n",
      "[133,   20] loss: 0.163\n",
      "[133,   40] loss: 0.295\n",
      "[134,   21] loss: 1.256\n",
      "[134,   42] loss: 0.901\n",
      "[135,   20] loss: 0.051\n",
      "[135,   40] loss: 0.033\n",
      "[136,   21] loss: 0.690\n",
      "[136,   42] loss: 0.640\n",
      "[137,   20] loss: 0.020\n",
      "[137,   40] loss: 0.017\n",
      "[138,   21] loss: 0.509\n",
      "[138,   42] loss: 0.464\n",
      "[139,   20] loss: 0.021\n",
      "[139,   40] loss: 0.018\n",
      "[140,   21] loss: 0.557\n",
      "[140,   42] loss: 0.459\n",
      "[141,   20] loss: 0.015\n",
      "[141,   40] loss: 0.013\n",
      "[142,   21] loss: 0.353\n",
      "[142,   42] loss: 0.391\n",
      "[143,   20] loss: 0.013\n",
      "[143,   40] loss: 0.014\n",
      "[144,   21] loss: 0.341\n",
      "[144,   42] loss: 0.293\n",
      "[145,   20] loss: 0.005\n",
      "[145,   40] loss: 0.004\n",
      "[146,   21] loss: 0.255\n",
      "[146,   42] loss: 0.220\n",
      "[147,   20] loss: 0.003\n",
      "[147,   40] loss: 0.003\n",
      "[148,   21] loss: 0.156\n",
      "[148,   42] loss: 0.169\n",
      "[149,   20] loss: 0.003\n",
      "[149,   40] loss: 0.005\n",
      "[150,   21] loss: 0.138\n",
      "[150,   42] loss: 0.163\n",
      "[151,   20] loss: 0.003\n",
      "[151,   40] loss: 0.003\n",
      "[152,   21] loss: 0.130\n",
      "[152,   42] loss: 0.146\n",
      "[153,   20] loss: 0.002\n",
      "[153,   40] loss: 0.002\n",
      "[154,   21] loss: 0.106\n",
      "[154,   42] loss: 0.099\n",
      "[155,   20] loss: 0.001\n",
      "[155,   40] loss: 0.001\n",
      "[156,   21] loss: 0.082\n",
      "[156,   42] loss: 0.077\n",
      "[157,   20] loss: 0.001\n",
      "[157,   40] loss: 0.001\n",
      "[158,   21] loss: 0.063\n",
      "[158,   42] loss: 0.084\n",
      "[159,   20] loss: 0.001\n",
      "[159,   40] loss: 0.000\n",
      "[160,   21] loss: 0.056\n",
      "[160,   42] loss: 0.065\n",
      "[161,   20] loss: 0.000\n",
      "[161,   40] loss: 0.000\n",
      "[162,   21] loss: 0.047\n",
      "[162,   42] loss: 0.049\n",
      "[163,   20] loss: 0.000\n",
      "[163,   40] loss: 0.000\n",
      "[164,   21] loss: 0.036\n",
      "[164,   42] loss: 0.040\n",
      "[165,   20] loss: 0.000\n",
      "[165,   40] loss: 0.000\n",
      "[166,   21] loss: 0.032\n",
      "[166,   42] loss: 0.035\n",
      "[167,   20] loss: 0.000\n",
      "[167,   40] loss: 0.000\n",
      "[168,   21] loss: 0.028\n",
      "[168,   42] loss: 0.031\n",
      "[169,   20] loss: 0.000\n",
      "[169,   40] loss: 0.000\n",
      "[170,   21] loss: 0.025\n",
      "[170,   42] loss: 0.028\n",
      "[171,   20] loss: 0.000\n",
      "[171,   40] loss: 0.000\n",
      "[172,   21] loss: 0.023\n",
      "[172,   42] loss: 0.025\n",
      "[173,   20] loss: 0.000\n",
      "[173,   40] loss: 0.000\n",
      "[174,   21] loss: 0.022\n",
      "[174,   42] loss: 0.024\n",
      "[175,   20] loss: 0.000\n",
      "[175,   40] loss: 0.000\n",
      "[176,   21] loss: 0.020\n",
      "[176,   42] loss: 0.022\n",
      "[177,   20] loss: 0.000\n",
      "[177,   40] loss: 0.000\n",
      "[178,   21] loss: 0.018\n",
      "[178,   42] loss: 0.020\n",
      "[179,   20] loss: 0.000\n",
      "[179,   40] loss: 0.000\n",
      "[180,   21] loss: 0.017\n",
      "[180,   42] loss: 0.019\n",
      "[181,   20] loss: 0.000\n",
      "[181,   40] loss: 0.000\n",
      "[182,   21] loss: 0.016\n",
      "[182,   42] loss: 0.018\n",
      "[183,   20] loss: 0.000\n",
      "[183,   40] loss: 0.000\n",
      "[184,   21] loss: 0.015\n",
      "[184,   42] loss: 0.017\n",
      "[185,   20] loss: 0.000\n",
      "[185,   40] loss: 0.000\n",
      "[186,   21] loss: 0.014\n",
      "[186,   42] loss: 0.016\n",
      "[187,   20] loss: 0.000\n",
      "[187,   40] loss: 0.000\n",
      "[188,   21] loss: 0.013\n",
      "[188,   42] loss: 0.015\n",
      "[189,   20] loss: 0.000\n",
      "[189,   40] loss: 0.000\n",
      "[190,   21] loss: 0.012\n",
      "[190,   42] loss: 0.014\n",
      "[191,   20] loss: 0.000\n",
      "[191,   40] loss: 0.000\n",
      "[192,   21] loss: 0.012\n",
      "[192,   42] loss: 0.013\n",
      "[193,   20] loss: 0.000\n",
      "[193,   40] loss: 0.000\n",
      "[194,   21] loss: 0.011\n",
      "[194,   42] loss: 0.013\n",
      "[195,   20] loss: 0.000\n",
      "[195,   40] loss: 0.000\n",
      "[196,   21] loss: 0.011\n",
      "[196,   42] loss: 0.012\n",
      "[197,   20] loss: 0.000\n",
      "[197,   40] loss: 0.000\n",
      "[198,   21] loss: 0.010\n",
      "[198,   42] loss: 0.012\n",
      "[199,   20] loss: 0.000\n",
      "[199,   40] loss: 0.000\n",
      "[200,   21] loss: 0.010\n",
      "[200,   42] loss: 0.012\n",
      "[201,   20] loss: 0.000\n",
      "[201,   40] loss: 0.000\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(201):\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        for param in cnnblock1.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        \n",
    "        for param in cnnblock2.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        \n",
    "        for param in cnnblock3.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        \n",
    "        for param in mlpblock1.parameters():\n",
    "            param.requires_grad_(True)\n",
    "            \n",
    "        for param in mlpblock2.parameters():\n",
    "            param.requires_grad_(True)\n",
    "            \n",
    "        for param in mlpblock3.parameters():\n",
    "            param.requires_grad_(True)\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(set2_loader,0):\n",
    "            inputs, set2labels = data\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = ensemblemodel(inputs)\n",
    "            loss = criterion(outputs, set2labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #print stats\n",
    "            running_loss += loss.item()\n",
    "            if i%20 == 19:\n",
    "                print('[%d,%5d] loss: %.3f' % (epoch+1, i + 1, running_loss / 20))\n",
    "                running_loss = 0.0\n",
    "                \n",
    "    else:\n",
    "        for param in cnnblock1.parameters():\n",
    "            param.requires_grad_(True)\n",
    "        \n",
    "        for param in cnnblock2.parameters():\n",
    "            param.requires_grad_(True)\n",
    "        \n",
    "        for param in cnnblock3.parameters():\n",
    "            param.requires_grad_(True)\n",
    "        \n",
    "        for param in mlpblock1.parameters():\n",
    "            param.requires_grad_(False)\n",
    "            \n",
    "        for param in mlpblock2.parameters():\n",
    "            param.requires_grad_(False)\n",
    "            \n",
    "        for param in mlpblock3.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, data in enumerate(set1_loader,0):\n",
    "            inputs, set1labels = data\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = ensemblemodel(inputs)\n",
    "            loss = criterion(outputs, set1labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #print stats\n",
    "            running_loss += loss.item()\n",
    "            if i%21 == 20:\n",
    "                print('[%d,%5d] loss: %.3f' % (epoch+1, i + 1, running_loss / 21))\n",
    "                running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Images Tested = 10000\n",
      "\n",
      "Model Accuracy = 0.3394\n"
     ]
    }
   ],
   "source": [
    "correct_count, all_count = 0, 0\n",
    "for inp,labels in test_loader:\n",
    "  for i in range(len(labels)):\n",
    "    with torch.no_grad():\n",
    "        logps = ensemblemodel(inp)\n",
    "\n",
    "    \n",
    "    ps = torch.exp(logps)\n",
    "    probab = list(ps.numpy()[0])\n",
    "    pred_label = probab.index(max(probab))\n",
    "    true_label = labels.numpy()[i]\n",
    "    if(true_label == pred_label):\n",
    "      correct_count += 1\n",
    "    all_count += 1\n",
    "\n",
    "print(\"Number Of Images Tested =\", all_count)\n",
    "print(\"\\nModel Accuracy =\", (correct_count/all_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EnsembleModel2, self).__init__()\n",
    "        self.cnnblock1 = cnnblock1\n",
    "        self.cnnblock2 = cnnblock2\n",
    "        self.cnnblock3 = cnnblock3\n",
    "        self.mlpblock1 = mlpblock1\n",
    "        self.mlpblock2 = mlpblock2\n",
    "        self.mlpblock3 = mlpblock3\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.cnnblock1(x)\n",
    "        x2 = self.cnnblock2(x1)\n",
    "        x3 = self.cnnblock3(x2)\n",
    "        x4 = self.mlpblock1(x1)\n",
    "        x5 = self.mlpblock2(x2)\n",
    "        x6 = self.mlpblock3(x3)\n",
    "        x7 = x4 + x5 + x6\n",
    "        return F.log_softmax(x7, dim=1)\n",
    "\n",
    "ensemblemodel2 = EnsembleModel2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer2 = optim.SGD(ensemblemodel2.parameters(), lr = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   20] loss: 2.287\n",
      "[1,   40] loss: 2.257\n",
      "[2,   21] loss: 2.227\n",
      "[2,   42] loss: 2.151\n",
      "[3,   20] loss: 2.102\n",
      "[3,   40] loss: 2.007\n",
      "[4,   21] loss: 2.103\n",
      "[4,   42] loss: 2.043\n",
      "[5,   20] loss: 1.984\n",
      "[5,   40] loss: 1.889\n",
      "[6,   21] loss: 2.005\n",
      "[6,   42] loss: 1.927\n",
      "[7,   20] loss: 1.857\n",
      "[7,   40] loss: 1.779\n",
      "[8,   21] loss: 1.893\n",
      "[8,   42] loss: 1.802\n",
      "[9,   20] loss: 1.706\n",
      "[9,   40] loss: 1.661\n",
      "[10,   21] loss: 1.774\n",
      "[10,   42] loss: 1.670\n",
      "[11,   20] loss: 1.552\n",
      "[11,   40] loss: 1.527\n",
      "[12,   21] loss: 1.627\n",
      "[12,   42] loss: 1.533\n",
      "[13,   20] loss: 1.395\n",
      "[13,   40] loss: 1.385\n",
      "[14,   21] loss: 1.444\n",
      "[14,   42] loss: 1.334\n",
      "[15,   20] loss: 1.208\n",
      "[15,   40] loss: 1.243\n",
      "[16,   21] loss: 1.243\n",
      "[16,   42] loss: 1.129\n",
      "[17,   20] loss: 1.099\n",
      "[17,   40] loss: 0.957\n",
      "[18,   21] loss: 1.039\n",
      "[18,   42] loss: 0.860\n",
      "[19,   20] loss: 0.790\n",
      "[19,   40] loss: 0.694\n",
      "[20,   21] loss: 0.766\n",
      "[20,   42] loss: 0.564\n",
      "[21,   20] loss: 0.587\n",
      "[21,   40] loss: 0.509\n",
      "[22,   21] loss: 0.513\n",
      "[22,   42] loss: 0.566\n",
      "[23,   20] loss: 0.364\n",
      "[23,   40] loss: 0.539\n",
      "[24,   21] loss: 0.484\n",
      "[24,   42] loss: 0.207\n",
      "[25,   20] loss: 0.476\n",
      "[25,   40] loss: 0.307\n",
      "[26,   21] loss: 0.140\n",
      "[26,   42] loss: 0.067\n",
      "[27,   20] loss: 0.105\n",
      "[27,   40] loss: 0.083\n",
      "[28,   21] loss: 0.259\n",
      "[28,   42] loss: 0.233\n",
      "[29,   20] loss: 0.057\n",
      "[29,   40] loss: 0.087\n",
      "[30,   21] loss: 0.066\n",
      "[30,   42] loss: 0.041\n",
      "[31,   20] loss: 0.518\n",
      "[31,   40] loss: 0.122\n",
      "[32,   21] loss: 0.053\n",
      "[32,   42] loss: 0.025\n",
      "[33,   20] loss: 0.047\n",
      "[33,   40] loss: 0.020\n",
      "[34,   21] loss: 0.017\n",
      "[34,   42] loss: 0.177\n",
      "[35,   20] loss: 0.082\n",
      "[35,   40] loss: 0.074\n",
      "[36,   21] loss: 0.085\n",
      "[36,   42] loss: 0.093\n",
      "[37,   20] loss: 0.205\n",
      "[37,   40] loss: 0.065\n",
      "[38,   21] loss: 0.052\n",
      "[38,   42] loss: 0.035\n",
      "[39,   20] loss: 0.014\n",
      "[39,   40] loss: 0.007\n",
      "[40,   21] loss: 0.007\n",
      "[40,   42] loss: 0.007\n",
      "[41,   20] loss: 0.005\n",
      "[41,   40] loss: 0.004\n",
      "[42,   21] loss: 0.003\n",
      "[42,   42] loss: 0.004\n",
      "[43,   20] loss: 0.002\n",
      "[43,   40] loss: 0.004\n",
      "[44,   21] loss: 0.002\n",
      "[44,   42] loss: 0.003\n",
      "[45,   20] loss: 0.001\n",
      "[45,   40] loss: 0.003\n",
      "[46,   21] loss: 0.001\n",
      "[46,   42] loss: 0.003\n",
      "[47,   20] loss: 0.001\n",
      "[47,   40] loss: 0.004\n",
      "[48,   21] loss: 0.001\n",
      "[48,   42] loss: 0.002\n",
      "[49,   20] loss: 0.001\n",
      "[49,   40] loss: 0.002\n",
      "[50,   21] loss: 0.001\n",
      "[50,   42] loss: 0.001\n",
      "[51,   20] loss: 0.001\n",
      "[51,   40] loss: 0.001\n",
      "[52,   21] loss: 0.001\n",
      "[52,   42] loss: 0.001\n",
      "[53,   20] loss: 0.001\n",
      "[53,   40] loss: 0.001\n",
      "[54,   21] loss: 0.001\n",
      "[54,   42] loss: 0.001\n",
      "[55,   20] loss: 0.000\n",
      "[55,   40] loss: 0.000\n",
      "[56,   21] loss: 0.001\n",
      "[56,   42] loss: 0.000\n",
      "[57,   20] loss: 0.000\n",
      "[57,   40] loss: 0.000\n",
      "[58,   21] loss: 0.000\n",
      "[58,   42] loss: 0.000\n",
      "[59,   20] loss: 0.000\n",
      "[59,   40] loss: 0.000\n",
      "[60,   21] loss: 0.000\n",
      "[60,   42] loss: 0.000\n",
      "[61,   20] loss: 0.000\n",
      "[61,   40] loss: 0.000\n",
      "[62,   21] loss: 0.000\n",
      "[62,   42] loss: 0.000\n",
      "[63,   20] loss: 0.000\n",
      "[63,   40] loss: 0.000\n",
      "[64,   21] loss: 0.000\n",
      "[64,   42] loss: 0.000\n",
      "[65,   20] loss: 0.000\n",
      "[65,   40] loss: 0.000\n",
      "[66,   21] loss: 0.000\n",
      "[66,   42] loss: 0.000\n",
      "[67,   20] loss: 0.000\n",
      "[67,   40] loss: 0.000\n",
      "[68,   21] loss: 0.000\n",
      "[68,   42] loss: 0.000\n",
      "[69,   20] loss: 0.000\n",
      "[69,   40] loss: 0.000\n",
      "[70,   21] loss: 0.000\n",
      "[70,   42] loss: 0.000\n",
      "[71,   20] loss: 0.000\n",
      "[71,   40] loss: 0.000\n",
      "[72,   21] loss: 0.000\n",
      "[72,   42] loss: 0.000\n",
      "[73,   20] loss: 0.000\n",
      "[73,   40] loss: 0.000\n",
      "[74,   21] loss: 0.000\n",
      "[74,   42] loss: 0.000\n",
      "[75,   20] loss: 0.000\n",
      "[75,   40] loss: 0.000\n",
      "[76,   21] loss: 0.000\n",
      "[76,   42] loss: 0.000\n",
      "[77,   20] loss: 0.000\n",
      "[77,   40] loss: 0.000\n",
      "[78,   21] loss: 0.000\n",
      "[78,   42] loss: 0.000\n",
      "[79,   20] loss: 0.000\n",
      "[79,   40] loss: 0.000\n",
      "[80,   21] loss: 0.000\n",
      "[80,   42] loss: 0.000\n",
      "[81,   20] loss: 0.000\n",
      "[81,   40] loss: 0.000\n",
      "[82,   21] loss: 0.000\n",
      "[82,   42] loss: 0.000\n",
      "[83,   20] loss: 0.000\n",
      "[83,   40] loss: 0.000\n",
      "[84,   21] loss: 0.000\n",
      "[84,   42] loss: 0.000\n",
      "[85,   20] loss: 0.000\n",
      "[85,   40] loss: 0.000\n",
      "[86,   21] loss: 0.000\n",
      "[86,   42] loss: 0.000\n",
      "[87,   20] loss: 0.000\n",
      "[87,   40] loss: 0.000\n",
      "[88,   21] loss: 0.000\n",
      "[88,   42] loss: 0.000\n",
      "[89,   20] loss: 0.000\n",
      "[89,   40] loss: 0.000\n",
      "[90,   21] loss: 0.000\n",
      "[90,   42] loss: 0.000\n",
      "[91,   20] loss: 0.000\n",
      "[91,   40] loss: 0.000\n",
      "[92,   21] loss: 0.000\n",
      "[92,   42] loss: 0.000\n",
      "[93,   20] loss: 0.000\n",
      "[93,   40] loss: 0.000\n",
      "[94,   21] loss: 0.000\n",
      "[94,   42] loss: 0.000\n",
      "[95,   20] loss: 0.000\n",
      "[95,   40] loss: 0.000\n",
      "[96,   21] loss: 0.000\n",
      "[96,   42] loss: 0.000\n",
      "[97,   20] loss: 0.000\n",
      "[97,   40] loss: 0.000\n",
      "[98,   21] loss: 0.000\n",
      "[98,   42] loss: 0.000\n",
      "[99,   20] loss: 0.000\n",
      "[99,   40] loss: 0.000\n",
      "[100,   21] loss: 0.000\n",
      "[100,   42] loss: 0.000\n",
      "[101,   20] loss: 0.000\n",
      "[101,   40] loss: 0.000\n",
      "[102,   21] loss: 0.000\n",
      "[102,   42] loss: 0.000\n",
      "[103,   20] loss: 0.000\n",
      "[103,   40] loss: 0.000\n",
      "[104,   21] loss: 0.000\n",
      "[104,   42] loss: 0.000\n",
      "[105,   20] loss: 0.000\n",
      "[105,   40] loss: 0.000\n",
      "[106,   21] loss: 0.000\n",
      "[106,   42] loss: 0.000\n",
      "[107,   20] loss: 0.000\n",
      "[107,   40] loss: 0.000\n",
      "[108,   21] loss: 0.000\n",
      "[108,   42] loss: 0.000\n",
      "[109,   20] loss: 0.000\n",
      "[109,   40] loss: 0.000\n",
      "[110,   21] loss: 0.000\n",
      "[110,   42] loss: 0.000\n",
      "[111,   20] loss: 0.000\n",
      "[111,   40] loss: 0.000\n",
      "[112,   21] loss: 0.000\n",
      "[112,   42] loss: 0.000\n",
      "[113,   20] loss: 0.000\n",
      "[113,   40] loss: 0.000\n",
      "[114,   21] loss: 0.000\n",
      "[114,   42] loss: 0.000\n",
      "[115,   20] loss: 0.000\n",
      "[115,   40] loss: 0.000\n",
      "[116,   21] loss: 0.000\n",
      "[116,   42] loss: 0.000\n",
      "[117,   20] loss: 0.000\n",
      "[117,   40] loss: 0.000\n",
      "[118,   21] loss: 0.000\n",
      "[118,   42] loss: 0.000\n",
      "[119,   20] loss: 0.000\n",
      "[119,   40] loss: 0.000\n",
      "[120,   21] loss: 0.000\n",
      "[120,   42] loss: 0.000\n",
      "[121,   20] loss: 0.000\n",
      "[121,   40] loss: 0.000\n",
      "[122,   21] loss: 0.000\n",
      "[122,   42] loss: 0.000\n",
      "[123,   20] loss: 0.000\n",
      "[123,   40] loss: 0.000\n",
      "[124,   21] loss: 0.000\n",
      "[124,   42] loss: 0.000\n",
      "[125,   20] loss: 0.000\n",
      "[125,   40] loss: 0.000\n",
      "[126,   21] loss: 0.000\n",
      "[126,   42] loss: 0.000\n",
      "[127,   20] loss: 0.000\n",
      "[127,   40] loss: 0.000\n",
      "[128,   21] loss: 0.000\n",
      "[128,   42] loss: 0.000\n",
      "[129,   20] loss: 0.000\n",
      "[129,   40] loss: 0.000\n",
      "[130,   21] loss: 0.000\n",
      "[130,   42] loss: 0.000\n",
      "[131,   20] loss: 0.000\n",
      "[131,   40] loss: 0.000\n",
      "[132,   21] loss: 0.000\n",
      "[132,   42] loss: 0.000\n",
      "[133,   20] loss: 0.000\n",
      "[133,   40] loss: 0.000\n",
      "[134,   21] loss: 0.000\n",
      "[134,   42] loss: 0.000\n",
      "[135,   20] loss: 0.000\n",
      "[135,   40] loss: 0.000\n",
      "[136,   21] loss: 0.000\n",
      "[136,   42] loss: 0.000\n",
      "[137,   20] loss: 0.000\n",
      "[137,   40] loss: 0.000\n",
      "[138,   21] loss: 0.000\n",
      "[138,   42] loss: 0.000\n",
      "[139,   20] loss: 0.000\n",
      "[139,   40] loss: 0.000\n",
      "[140,   21] loss: 0.000\n",
      "[140,   42] loss: 0.000\n",
      "[141,   20] loss: 0.000\n",
      "[141,   40] loss: 0.000\n",
      "[142,   21] loss: 0.000\n",
      "[142,   42] loss: 0.000\n",
      "[143,   20] loss: 0.000\n",
      "[143,   40] loss: 0.000\n",
      "[144,   21] loss: 0.000\n",
      "[144,   42] loss: 0.000\n",
      "[145,   20] loss: 0.000\n",
      "[145,   40] loss: 0.000\n",
      "[146,   21] loss: 0.000\n",
      "[146,   42] loss: 0.000\n",
      "[147,   20] loss: 0.000\n",
      "[147,   40] loss: 0.000\n",
      "[148,   21] loss: 0.000\n",
      "[148,   42] loss: 0.000\n",
      "[149,   20] loss: 0.000\n",
      "[149,   40] loss: 0.000\n",
      "[150,   21] loss: 0.000\n",
      "[150,   42] loss: 0.000\n",
      "[151,   20] loss: 0.000\n",
      "[151,   40] loss: 0.000\n",
      "[152,   21] loss: 0.000\n",
      "[152,   42] loss: 0.000\n",
      "[153,   20] loss: 0.000\n",
      "[153,   40] loss: 0.000\n",
      "[154,   21] loss: 0.000\n",
      "[154,   42] loss: 0.000\n",
      "[155,   20] loss: 0.000\n",
      "[155,   40] loss: 0.000\n",
      "[156,   21] loss: 0.000\n",
      "[156,   42] loss: 0.000\n",
      "[157,   20] loss: 0.000\n",
      "[157,   40] loss: 0.000\n",
      "[158,   21] loss: 0.000\n",
      "[158,   42] loss: 0.000\n",
      "[159,   20] loss: 0.000\n",
      "[159,   40] loss: 0.000\n",
      "[160,   21] loss: 0.000\n",
      "[160,   42] loss: 0.000\n",
      "[161,   20] loss: 0.000\n",
      "[161,   40] loss: 0.000\n",
      "[162,   21] loss: 0.000\n",
      "[162,   42] loss: 0.000\n",
      "[163,   20] loss: 0.000\n",
      "[163,   40] loss: 0.000\n",
      "[164,   21] loss: 0.000\n",
      "[164,   42] loss: 0.000\n",
      "[165,   20] loss: 0.000\n",
      "[165,   40] loss: 0.000\n",
      "[166,   21] loss: 0.000\n",
      "[166,   42] loss: 0.000\n",
      "[167,   20] loss: 0.000\n",
      "[167,   40] loss: 0.000\n",
      "[168,   21] loss: 0.000\n",
      "[168,   42] loss: 0.000\n",
      "[169,   20] loss: 0.000\n",
      "[169,   40] loss: 0.000\n",
      "[170,   21] loss: 0.000\n",
      "[170,   42] loss: 0.000\n",
      "[171,   20] loss: 0.000\n",
      "[171,   40] loss: 0.000\n",
      "[172,   21] loss: 0.000\n",
      "[172,   42] loss: 0.000\n",
      "[173,   20] loss: 0.000\n",
      "[173,   40] loss: 0.000\n",
      "[174,   21] loss: 0.000\n",
      "[174,   42] loss: 0.000\n",
      "[175,   20] loss: 0.000\n",
      "[175,   40] loss: 0.000\n",
      "[176,   21] loss: 0.000\n",
      "[176,   42] loss: 0.000\n",
      "[177,   20] loss: 0.000\n",
      "[177,   40] loss: 0.000\n",
      "[178,   21] loss: 0.000\n",
      "[178,   42] loss: 0.000\n",
      "[179,   20] loss: 0.000\n",
      "[179,   40] loss: 0.000\n",
      "[180,   21] loss: 0.000\n",
      "[180,   42] loss: 0.000\n",
      "[181,   20] loss: 0.000\n",
      "[181,   40] loss: 0.000\n",
      "[182,   21] loss: 0.000\n",
      "[182,   42] loss: 0.000\n",
      "[183,   20] loss: 0.000\n",
      "[183,   40] loss: 0.000\n",
      "[184,   21] loss: 0.000\n",
      "[184,   42] loss: 0.000\n",
      "[185,   20] loss: 0.000\n",
      "[185,   40] loss: 0.000\n",
      "[186,   21] loss: 0.000\n",
      "[186,   42] loss: 0.000\n",
      "[187,   20] loss: 0.000\n",
      "[187,   40] loss: 0.000\n",
      "[188,   21] loss: 0.000\n",
      "[188,   42] loss: 0.000\n",
      "[189,   20] loss: 0.000\n",
      "[189,   40] loss: 0.000\n",
      "[190,   21] loss: 0.000\n",
      "[190,   42] loss: 0.000\n",
      "[191,   20] loss: 0.000\n",
      "[191,   40] loss: 0.000\n",
      "[192,   21] loss: 0.000\n",
      "[192,   42] loss: 0.000\n",
      "[193,   20] loss: 0.000\n",
      "[193,   40] loss: 0.000\n",
      "[194,   21] loss: 0.000\n",
      "[194,   42] loss: 0.000\n",
      "[195,   20] loss: 0.000\n",
      "[195,   40] loss: 0.000\n",
      "[196,   21] loss: 0.000\n",
      "[196,   42] loss: 0.000\n",
      "[197,   20] loss: 0.000\n",
      "[197,   40] loss: 0.000\n",
      "[198,   21] loss: 0.000\n",
      "[198,   42] loss: 0.000\n",
      "[199,   20] loss: 0.000\n",
      "[199,   40] loss: 0.000\n",
      "[200,   21] loss: 0.000\n",
      "[200,   42] loss: 0.000\n",
      "[201,   20] loss: 0.000\n",
      "[201,   40] loss: 0.000\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(201):\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        for param in cnnblock1.parameters():\n",
    "            param.requires_grad_(True)\n",
    "        \n",
    "        for param in cnnblock2.parameters():\n",
    "            param.requires_grad_(True)\n",
    "        \n",
    "        for param in cnnblock3.parameters():\n",
    "            param.requires_grad_(True)\n",
    "        \n",
    "        for param in mlpblock1.parameters():\n",
    "            param.requires_grad_(True)\n",
    "            \n",
    "        for param in mlpblock2.parameters():\n",
    "            param.requires_grad_(True)\n",
    "            \n",
    "        for param in mlpblock3.parameters():\n",
    "            param.requires_grad_(True)\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(set2_loader,0):\n",
    "            inputs, set2labels = data\n",
    "            \n",
    "            optimizer2.zero_grad()\n",
    "            \n",
    "            outputs = ensemblemodel2(inputs)\n",
    "            loss = criterion(outputs, set2labels)\n",
    "            loss.backward()\n",
    "            optimizer2.step()\n",
    "            \n",
    "            #print stats\n",
    "            running_loss += loss.item()\n",
    "            if i%20 == 19:\n",
    "                print('[%d,%5d] loss: %.3f' % (epoch+1, i + 1, running_loss / 20))\n",
    "                running_loss = 0.0\n",
    "                \n",
    "    else:\n",
    "        for param in cnnblock1.parameters():\n",
    "            param.requires_grad_(True)\n",
    "        \n",
    "        for param in cnnblock2.parameters():\n",
    "            param.requires_grad_(True)\n",
    "        \n",
    "        for param in cnnblock3.parameters():\n",
    "            param.requires_grad_(True)\n",
    "        \n",
    "        for param in mlpblock1.parameters():\n",
    "            param.requires_grad_(True)\n",
    "            \n",
    "        for param in mlpblock2.parameters():\n",
    "            param.requires_grad_(True)\n",
    "            \n",
    "        for param in mlpblock3.parameters():\n",
    "            param.requires_grad_(True)\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, data in enumerate(set1_loader,0):\n",
    "            inputs, set1labels = data\n",
    "            \n",
    "            optimizer2.zero_grad()\n",
    "            \n",
    "            outputs = ensemblemodel2(inputs)\n",
    "            loss = criterion(outputs, set1labels)\n",
    "            loss.backward()\n",
    "            optimizer2.step()\n",
    "            \n",
    "            #print stats\n",
    "            running_loss += loss.item()\n",
    "            if i%21 == 20:\n",
    "                print('[%d,%5d] loss: %.3f' % (epoch+1, i + 1, running_loss / 21))\n",
    "                running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Images Tested = 10000\n",
      "\n",
      "Model Accuracy = 0.4041\n"
     ]
    }
   ],
   "source": [
    "correct_count, all_count = 0, 0\n",
    "for inp,labels in test_loader:\n",
    "  for i in range(len(labels)):\n",
    "    with torch.no_grad():\n",
    "        logps = ensemblemodel2(inp)\n",
    "\n",
    "    \n",
    "    ps = torch.exp(logps)\n",
    "    probab = list(ps.numpy()[0])\n",
    "    pred_label = probab.index(max(probab))\n",
    "    true_label = labels.numpy()[i]\n",
    "    if(true_label == pred_label):\n",
    "      correct_count += 1\n",
    "    all_count += 1\n",
    "\n",
    "print(\"Number Of Images Tested =\", all_count)\n",
    "print(\"\\nModel Accuracy =\", (correct_count/all_count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
