{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 10000\n"
     ]
    }
   ],
   "source": [
    "train_len = len(trainset)\n",
    "test_len = len(testset)\n",
    "index = list(range(train_len))\n",
    "print(train_len, test_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct validation set (lets use 10 percent)\n",
    "np.random.shuffle(index)\n",
    "#number of blocks of data\n",
    "split = int(0.1 * train_len)\n",
    "train_index = index[split:]\n",
    "validation_index = index[:split]\n",
    "#Need to use a dataloader to control batch size and also enable SGD\n",
    "train_loader = torch.utils.data.DataLoader(trainset, sampler = train_index, batch_size = 4, num_workers = 10)\n",
    "validation_loader = torch.utils.data.DataLoader(trainset, sampler = validation_index)\n",
    "test_loader = torch.utils.data.DataLoader(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindataiter = iter(train_loader)\n",
    "trainimages, trainlabels = traindataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelA, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(15*15*128, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x),2))\n",
    "        x = x.view(-1,15*15*128 )\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "modelA = ModelA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(modelA.parameters(), lr = 0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2000] loss: 2.002\n",
      "[1, 4000] loss: 1.667\n",
      "[1, 6000] loss: 1.523\n",
      "[1, 8000] loss: 1.418\n",
      "[1,10000] loss: 1.357\n",
      "[2, 2000] loss: 1.262\n",
      "[2, 4000] loss: 1.193\n",
      "[2, 6000] loss: 1.155\n",
      "[2, 8000] loss: 1.103\n",
      "[2,10000] loss: 1.073\n",
      "[3, 2000] loss: 1.011\n",
      "[3, 4000] loss: 0.965\n",
      "[3, 6000] loss: 0.931\n",
      "[3, 8000] loss: 0.892\n",
      "[3,10000] loss: 0.873\n",
      "[4, 2000] loss: 0.826\n",
      "[4, 4000] loss: 0.790\n",
      "[4, 6000] loss: 0.756\n",
      "[4, 8000] loss: 0.726\n",
      "[4,10000] loss: 0.706\n",
      "[5, 2000] loss: 0.662\n",
      "[5, 4000] loss: 0.628\n",
      "[5, 6000] loss: 0.591\n",
      "[5, 8000] loss: 0.564\n",
      "[5,10000] loss: 0.541\n",
      "[6, 2000] loss: 0.494\n",
      "[6, 4000] loss: 0.459\n",
      "[6, 6000] loss: 0.420\n",
      "[6, 8000] loss: 0.387\n",
      "[6,10000] loss: 0.365\n",
      "[7, 2000] loss: 0.314\n",
      "[7, 4000] loss: 0.285\n",
      "[7, 6000] loss: 0.245\n",
      "[7, 8000] loss: 0.212\n",
      "[7,10000] loss: 0.202\n",
      "[8, 2000] loss: 0.160\n",
      "[8, 4000] loss: 0.143\n",
      "[8, 6000] loss: 0.135\n",
      "[8, 8000] loss: 0.112\n",
      "[8,10000] loss: 0.117\n",
      "[9, 2000] loss: 0.098\n",
      "[9, 4000] loss: 0.094\n",
      "[9, 6000] loss: 0.102\n",
      "[9, 8000] loss: 0.093\n",
      "[9,10000] loss: 0.092\n",
      "[10, 2000] loss: 0.092\n",
      "[10, 4000] loss: 0.096\n",
      "[10, 6000] loss: 0.083\n",
      "[10, 8000] loss: 0.083\n",
      "[10,10000] loss: 0.076\n",
      "[11, 2000] loss: 0.061\n",
      "[11, 4000] loss: 0.067\n",
      "[11, 6000] loss: 0.063\n",
      "[11, 8000] loss: 0.050\n",
      "[11,10000] loss: 0.055\n",
      "[12, 2000] loss: 0.057\n",
      "[12, 4000] loss: 0.048\n",
      "[12, 6000] loss: 0.044\n",
      "[12, 8000] loss: 0.047\n",
      "[12,10000] loss: 0.040\n",
      "[13, 2000] loss: 0.037\n",
      "[13, 4000] loss: 0.036\n",
      "[13, 6000] loss: 0.028\n",
      "[13, 8000] loss: 0.029\n",
      "[13,10000] loss: 0.032\n",
      "[14, 2000] loss: 0.026\n",
      "[14, 4000] loss: 0.024\n",
      "[14, 6000] loss: 0.018\n",
      "[14, 8000] loss: 0.017\n",
      "[14,10000] loss: 0.015\n",
      "[15, 2000] loss: 0.019\n",
      "[15, 4000] loss: 0.020\n",
      "[15, 6000] loss: 0.014\n",
      "[15, 8000] loss: 0.013\n",
      "[15,10000] loss: 0.012\n",
      "[16, 2000] loss: 0.010\n",
      "[16, 4000] loss: 0.008\n",
      "[16, 6000] loss: 0.008\n",
      "[16, 8000] loss: 0.007\n",
      "[16,10000] loss: 0.008\n",
      "[17, 2000] loss: 0.006\n",
      "[17, 4000] loss: 0.004\n",
      "[17, 6000] loss: 0.006\n",
      "[17, 8000] loss: 0.005\n",
      "[17,10000] loss: 0.004\n",
      "[18, 2000] loss: 0.002\n",
      "[18, 4000] loss: 0.002\n",
      "[18, 6000] loss: 0.003\n",
      "[18, 8000] loss: 0.002\n",
      "[18,10000] loss: 0.001\n",
      "[19, 2000] loss: 0.001\n",
      "[19, 4000] loss: 0.001\n",
      "[19, 6000] loss: 0.001\n",
      "[19, 8000] loss: 0.001\n",
      "[19,10000] loss: 0.001\n",
      "[20, 2000] loss: 0.001\n",
      "[20, 4000] loss: 0.001\n",
      "[20, 6000] loss: 0.000\n",
      "[20, 8000] loss: 0.000\n",
      "[20,10000] loss: 0.000\n",
      "[21, 2000] loss: 0.000\n",
      "[21, 4000] loss: 0.000\n",
      "[21, 6000] loss: 0.000\n",
      "[21, 8000] loss: 0.000\n",
      "[21,10000] loss: 0.000\n",
      "[22, 2000] loss: 0.000\n",
      "[22, 4000] loss: 0.000\n",
      "[22, 6000] loss: 0.000\n",
      "[22, 8000] loss: 0.000\n",
      "[22,10000] loss: 0.000\n",
      "[23, 2000] loss: 0.000\n",
      "[23, 4000] loss: 0.000\n",
      "[23, 6000] loss: 0.000\n",
      "[23, 8000] loss: 0.000\n",
      "[23,10000] loss: 0.000\n",
      "[24, 2000] loss: 0.000\n",
      "[24, 4000] loss: 0.000\n",
      "[24, 6000] loss: 0.000\n",
      "[24, 8000] loss: 0.000\n",
      "[24,10000] loss: 0.000\n",
      "[25, 2000] loss: 0.000\n",
      "[25, 4000] loss: 0.000\n",
      "[25, 6000] loss: 0.000\n",
      "[25, 8000] loss: 0.000\n",
      "[25,10000] loss: 0.000\n",
      "[26, 2000] loss: 0.000\n",
      "[26, 4000] loss: 0.000\n",
      "[26, 6000] loss: 0.000\n",
      "[26, 8000] loss: 0.000\n",
      "[26,10000] loss: 0.000\n",
      "[27, 2000] loss: 0.000\n",
      "[27, 4000] loss: 0.000\n",
      "[27, 6000] loss: 0.000\n",
      "[27, 8000] loss: 0.000\n",
      "[27,10000] loss: 0.000\n",
      "[28, 2000] loss: 0.000\n",
      "[28, 4000] loss: 0.000\n",
      "[28, 6000] loss: 0.000\n",
      "[28, 8000] loss: 0.000\n",
      "[28,10000] loss: 0.000\n",
      "[29, 2000] loss: 0.000\n",
      "[29, 4000] loss: 0.000\n",
      "[29, 6000] loss: 0.000\n",
      "[29, 8000] loss: 0.000\n",
      "[29,10000] loss: 0.000\n",
      "[30, 2000] loss: 0.000\n",
      "[30, 4000] loss: 0.000\n",
      "[30, 6000] loss: 0.000\n",
      "[30, 8000] loss: 0.000\n",
      "[30,10000] loss: 0.000\n",
      "[31, 2000] loss: 0.000\n",
      "[31, 4000] loss: 0.000\n",
      "[31, 6000] loss: 0.000\n",
      "[31, 8000] loss: 0.000\n",
      "[31,10000] loss: 0.000\n",
      "[32, 2000] loss: 0.000\n",
      "[32, 4000] loss: 0.000\n",
      "[32, 6000] loss: 0.000\n",
      "[32, 8000] loss: 0.000\n",
      "[32,10000] loss: 0.000\n",
      "[33, 2000] loss: 0.000\n",
      "[33, 4000] loss: 0.000\n",
      "[33, 6000] loss: 0.000\n",
      "[33, 8000] loss: 0.000\n",
      "[33,10000] loss: 0.000\n",
      "[34, 2000] loss: 0.000\n",
      "[34, 4000] loss: 0.000\n",
      "[34, 6000] loss: 0.000\n",
      "[34, 8000] loss: 0.000\n",
      "[34,10000] loss: 0.000\n",
      "[35, 2000] loss: 0.000\n",
      "[35, 4000] loss: 0.000\n",
      "[35, 6000] loss: 0.000\n",
      "[35, 8000] loss: 0.000\n",
      "[35,10000] loss: 0.000\n",
      "[36, 2000] loss: 0.000\n",
      "[36, 4000] loss: 0.000\n",
      "[36, 6000] loss: 0.000\n",
      "[36, 8000] loss: 0.000\n",
      "[36,10000] loss: 0.000\n",
      "[37, 2000] loss: 0.000\n",
      "[37, 4000] loss: 0.000\n",
      "[37, 6000] loss: 0.000\n",
      "[37, 8000] loss: 0.000\n",
      "[37,10000] loss: 0.000\n",
      "[38, 2000] loss: 0.000\n",
      "[38, 4000] loss: 0.000\n",
      "[38, 6000] loss: 0.000\n",
      "[38, 8000] loss: 0.000\n",
      "[38,10000] loss: 0.000\n",
      "[39, 2000] loss: 0.000\n",
      "[39, 4000] loss: 0.000\n",
      "[39, 6000] loss: 0.000\n",
      "[39, 8000] loss: 0.000\n",
      "[39,10000] loss: 0.000\n",
      "[40, 2000] loss: 0.000\n",
      "[40, 4000] loss: 0.000\n",
      "[40, 6000] loss: 0.000\n",
      "[40, 8000] loss: 0.000\n",
      "[40,10000] loss: 0.000\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(40): #loop over the data set \n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader,0):\n",
    "        #get inputs; data is list of [inputs,labels]\n",
    "        inputs, trainlabels = data\n",
    "        \n",
    "        #zero param gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #forward + backward + optimize\n",
    "        outputs = modelA(inputs)\n",
    "        loss = criterion(outputs, trainlabels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #print stats\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999: #print every 2000 mini-batches\n",
    "            print('[%d,%5d] loss: %.3f' % (epoch+1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Images Tested = 10000\n",
      "\n",
      "Model Accuracy = 0.7152\n"
     ]
    }
   ],
   "source": [
    "# To test modelA on the data\n",
    "# getting predictions on test set and measuring the performance\n",
    "correct_count, all_count = 0, 0\n",
    "for inp,labels in test_loader:\n",
    "  for i in range(len(labels)):\n",
    "    with torch.no_grad():\n",
    "        logps = modelA(inp)\n",
    "\n",
    "    \n",
    "    ps = torch.exp(logps)\n",
    "    probab = list(ps.numpy()[0])\n",
    "    pred_label = probab.index(max(probab))\n",
    "    true_label = labels.numpy()[i]\n",
    "    if(true_label == pred_label):\n",
    "      correct_count += 1\n",
    "    all_count += 1\n",
    "\n",
    "print(\"Number Of Images Tested =\", all_count)\n",
    "print(\"\\nModel Accuracy =\", (correct_count/all_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(modelA.state_dict(), '/home/brian_chen/modelA.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelA = ModelA()\n",
    "modelA.load_state_dict(torch.load('/home/brian_chen/modelA.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelB, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size = 3, padding = 1)\n",
    "        self.conv4 = nn.Conv2d(128,256, kernel_size = 4, padding = 1)\n",
    "        self.fc1 = nn.Linear(7*7*256, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x),2))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(F.max_pool2d(self.conv4(x),2))\n",
    "        x = x.view(-1,7*7*256 )\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "modelB = ModelB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(modelB.parameters(), lr = 0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2000] loss: 2.268\n",
      "[1, 4000] loss: 1.968\n",
      "[1, 6000] loss: 1.784\n",
      "[1, 8000] loss: 1.637\n",
      "[1,10000] loss: 1.557\n",
      "[2, 2000] loss: 1.448\n",
      "[2, 4000] loss: 1.368\n",
      "[2, 6000] loss: 1.320\n",
      "[2, 8000] loss: 1.256\n",
      "[2,10000] loss: 1.222\n",
      "[3, 2000] loss: 1.139\n",
      "[3, 4000] loss: 1.071\n",
      "[3, 6000] loss: 1.040\n",
      "[3, 8000] loss: 0.988\n",
      "[3,10000] loss: 0.963\n",
      "[4, 2000] loss: 0.909\n",
      "[4, 4000] loss: 0.860\n",
      "[4, 6000] loss: 0.838\n",
      "[4, 8000] loss: 0.793\n",
      "[4,10000] loss: 0.780\n",
      "[5, 2000] loss: 0.736\n",
      "[5, 4000] loss: 0.698\n",
      "[5, 6000] loss: 0.672\n",
      "[5, 8000] loss: 0.630\n",
      "[5,10000] loss: 0.620\n",
      "[6, 2000] loss: 0.576\n",
      "[6, 4000] loss: 0.539\n",
      "[6, 6000] loss: 0.509\n",
      "[6, 8000] loss: 0.467\n",
      "[6,10000] loss: 0.459\n",
      "[7, 2000] loss: 0.404\n",
      "[7, 4000] loss: 0.375\n",
      "[7, 6000] loss: 0.346\n",
      "[7, 8000] loss: 0.302\n",
      "[7,10000] loss: 0.294\n",
      "[8, 2000] loss: 0.242\n",
      "[8, 4000] loss: 0.230\n",
      "[8, 6000] loss: 0.226\n",
      "[8, 8000] loss: 0.198\n",
      "[8,10000] loss: 0.193\n",
      "[9, 2000] loss: 0.163\n",
      "[9, 4000] loss: 0.161\n",
      "[9, 6000] loss: 0.163\n",
      "[9, 8000] loss: 0.153\n",
      "[9,10000] loss: 0.164\n",
      "[10, 2000] loss: 0.144\n",
      "[10, 4000] loss: 0.122\n",
      "[10, 6000] loss: 0.128\n",
      "[10, 8000] loss: 0.105\n",
      "[10,10000] loss: 0.116\n",
      "[11, 2000] loss: 0.095\n",
      "[11, 4000] loss: 0.080\n",
      "[11, 6000] loss: 0.081\n",
      "[11, 8000] loss: 0.083\n",
      "[11,10000] loss: 0.077\n",
      "[12, 2000] loss: 0.078\n",
      "[12, 4000] loss: 0.071\n",
      "[12, 6000] loss: 0.067\n",
      "[12, 8000] loss: 0.062\n",
      "[12,10000] loss: 0.061\n",
      "[13, 2000] loss: 0.064\n",
      "[13, 4000] loss: 0.052\n",
      "[13, 6000] loss: 0.052\n",
      "[13, 8000] loss: 0.043\n",
      "[13,10000] loss: 0.045\n",
      "[14, 2000] loss: 0.046\n",
      "[14, 4000] loss: 0.057\n",
      "[14, 6000] loss: 0.046\n",
      "[14, 8000] loss: 0.048\n",
      "[14,10000] loss: 0.048\n",
      "[15, 2000] loss: 0.036\n",
      "[15, 4000] loss: 0.038\n",
      "[15, 6000] loss: 0.030\n",
      "[15, 8000] loss: 0.033\n",
      "[15,10000] loss: 0.033\n",
      "[16, 2000] loss: 0.023\n",
      "[16, 4000] loss: 0.021\n",
      "[16, 6000] loss: 0.025\n",
      "[16, 8000] loss: 0.030\n",
      "[16,10000] loss: 0.026\n",
      "[17, 2000] loss: 0.022\n",
      "[17, 4000] loss: 0.018\n",
      "[17, 6000] loss: 0.027\n",
      "[17, 8000] loss: 0.025\n",
      "[17,10000] loss: 0.025\n",
      "[18, 2000] loss: 0.023\n",
      "[18, 4000] loss: 0.024\n",
      "[18, 6000] loss: 0.024\n",
      "[18, 8000] loss: 0.022\n",
      "[18,10000] loss: 0.023\n",
      "[19, 2000] loss: 0.019\n",
      "[19, 4000] loss: 0.020\n",
      "[19, 6000] loss: 0.023\n",
      "[19, 8000] loss: 0.012\n",
      "[19,10000] loss: 0.013\n",
      "[20, 2000] loss: 0.015\n",
      "[20, 4000] loss: 0.017\n",
      "[20, 6000] loss: 0.013\n",
      "[20, 8000] loss: 0.015\n",
      "[20,10000] loss: 0.019\n",
      "[21, 2000] loss: 0.032\n",
      "[21, 4000] loss: 0.019\n",
      "[21, 6000] loss: 0.020\n",
      "[21, 8000] loss: 0.022\n",
      "[21,10000] loss: 0.020\n",
      "[22, 2000] loss: 0.015\n",
      "[22, 4000] loss: 0.017\n",
      "[22, 6000] loss: 0.015\n",
      "[22, 8000] loss: 0.010\n",
      "[22,10000] loss: 0.014\n",
      "[23, 2000] loss: 0.015\n",
      "[23, 4000] loss: 0.008\n",
      "[23, 6000] loss: 0.011\n",
      "[23, 8000] loss: 0.009\n",
      "[23,10000] loss: 0.010\n",
      "[24, 2000] loss: 0.010\n",
      "[24, 4000] loss: 0.006\n",
      "[24, 6000] loss: 0.004\n",
      "[24, 8000] loss: 0.005\n",
      "[24,10000] loss: 0.004\n",
      "[25, 2000] loss: 0.008\n",
      "[25, 4000] loss: 0.008\n",
      "[25, 6000] loss: 0.011\n",
      "[25, 8000] loss: 0.008\n",
      "[25,10000] loss: 0.009\n",
      "[26, 2000] loss: 0.004\n",
      "[26, 4000] loss: 0.004\n",
      "[26, 6000] loss: 0.003\n",
      "[26, 8000] loss: 0.003\n",
      "[26,10000] loss: 0.004\n",
      "[27, 2000] loss: 0.003\n",
      "[27, 4000] loss: 0.003\n",
      "[27, 6000] loss: 0.002\n",
      "[27, 8000] loss: 0.001\n",
      "[27,10000] loss: 0.001\n",
      "[28, 2000] loss: 0.000\n",
      "[28, 4000] loss: 0.000\n",
      "[28, 6000] loss: 0.000\n",
      "[28, 8000] loss: 0.000\n",
      "[28,10000] loss: 0.000\n",
      "[29, 2000] loss: 0.000\n",
      "[29, 4000] loss: 0.000\n",
      "[29, 6000] loss: 0.000\n",
      "[29, 8000] loss: 0.000\n",
      "[29,10000] loss: 0.000\n",
      "[30, 2000] loss: 0.000\n",
      "[30, 4000] loss: 0.000\n",
      "[30, 6000] loss: 0.000\n",
      "[30, 8000] loss: 0.000\n",
      "[30,10000] loss: 0.000\n",
      "[31, 2000] loss: 0.000\n",
      "[31, 4000] loss: 0.000\n",
      "[31, 6000] loss: 0.000\n",
      "[31, 8000] loss: 0.000\n",
      "[31,10000] loss: 0.000\n",
      "[32, 2000] loss: 0.000\n",
      "[32, 4000] loss: 0.000\n",
      "[32, 6000] loss: 0.000\n",
      "[32, 8000] loss: 0.000\n",
      "[32,10000] loss: 0.000\n",
      "[33, 2000] loss: 0.000\n",
      "[33, 4000] loss: 0.000\n",
      "[33, 6000] loss: 0.000\n",
      "[33, 8000] loss: 0.000\n",
      "[33,10000] loss: 0.000\n",
      "[34, 2000] loss: 0.000\n",
      "[34, 4000] loss: 0.000\n",
      "[34, 6000] loss: 0.000\n",
      "[34, 8000] loss: 0.000\n",
      "[34,10000] loss: 0.000\n",
      "[35, 2000] loss: 0.000\n",
      "[35, 4000] loss: 0.000\n",
      "[35, 6000] loss: 0.000\n",
      "[35, 8000] loss: 0.000\n",
      "[35,10000] loss: 0.000\n",
      "[36, 2000] loss: 0.000\n",
      "[36, 4000] loss: 0.000\n",
      "[36, 6000] loss: 0.000\n",
      "[36, 8000] loss: 0.000\n",
      "[36,10000] loss: 0.000\n",
      "[37, 2000] loss: 0.000\n",
      "[37, 4000] loss: 0.000\n",
      "[37, 6000] loss: 0.000\n",
      "[37, 8000] loss: 0.000\n",
      "[37,10000] loss: 0.000\n",
      "[38, 2000] loss: 0.000\n",
      "[38, 4000] loss: 0.000\n",
      "[38, 6000] loss: 0.000\n",
      "[38, 8000] loss: 0.000\n",
      "[38,10000] loss: 0.000\n",
      "[39, 2000] loss: 0.000\n",
      "[39, 4000] loss: 0.000\n",
      "[39, 6000] loss: 0.000\n",
      "[39, 8000] loss: 0.000\n",
      "[39,10000] loss: 0.000\n",
      "[40, 2000] loss: 0.000\n",
      "[40, 4000] loss: 0.000\n",
      "[40, 6000] loss: 0.000\n",
      "[40, 8000] loss: 0.000\n",
      "[40,10000] loss: 0.000\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(40): #loop over the data set \n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader,0):\n",
    "        #get inputs; data is list of [inputs,labels]\n",
    "        inputs, trainlabels = data\n",
    "        \n",
    "        #zero param gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #forward + backward + optimize\n",
    "        outputs = modelB(inputs)\n",
    "        loss = criterion(outputs, trainlabels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #print stats\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999: #print every 2000 mini-batches\n",
    "            print('[%d,%5d] loss: %.3f' % (epoch+1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Images Tested = 10000\n",
      "\n",
      "Model Accuracy = 0.7672\n"
     ]
    }
   ],
   "source": [
    "# To test modelB on the data\n",
    "# getting predictions on test set and measuring the performance\n",
    "correct_count, all_count = 0, 0\n",
    "for inp,labels in test_loader:\n",
    "  for i in range(len(labels)):\n",
    "    with torch.no_grad():\n",
    "        logps = modelB(inp)\n",
    "\n",
    "    \n",
    "    ps = torch.exp(logps)\n",
    "    probab = list(ps.numpy()[0])\n",
    "    pred_label = probab.index(max(probab))\n",
    "    true_label = labels.numpy()[i]\n",
    "    if(true_label == pred_label):\n",
    "      correct_count += 1\n",
    "    all_count += 1\n",
    "\n",
    "print(\"Number Of Images Tested =\", all_count)\n",
    "print(\"\\nModel Accuracy =\", (correct_count/all_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(modelB.state_dict(), '/home/brian_chen/modelB.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelB = ModelB()\n",
    "modelB.load_state_dict(torch.load('/home/brian_chen/modelB.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelC, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size = 3, padding = 1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size = 4, padding = 1)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size = 3, padding = 1)\n",
    "        self.conv6 = nn.Conv2d(256,512, kernel_size = 2)\n",
    "        self.fc1 = nn.Linear(6*6*512, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x),2))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(F.max_pool2d(self.conv4(x),2))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = x.view(-1,6*6*512)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "modelC = ModelC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(modelC.parameters(), lr = 0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2000] loss: 2.303\n",
      "[1, 4000] loss: 2.302\n",
      "[1, 6000] loss: 2.252\n",
      "[1, 8000] loss: 2.016\n",
      "[1,10000] loss: 1.873\n",
      "[2, 2000] loss: 1.714\n",
      "[2, 4000] loss: 1.620\n",
      "[2, 6000] loss: 1.546\n",
      "[2, 8000] loss: 1.483\n",
      "[2,10000] loss: 1.456\n",
      "[3, 2000] loss: 1.381\n",
      "[3, 4000] loss: 1.326\n",
      "[3, 6000] loss: 1.283\n",
      "[3, 8000] loss: 1.243\n",
      "[3,10000] loss: 1.235\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(40): #loop over the data set \n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader,0):\n",
    "        #get inputs; data is list of [inputs,labels]\n",
    "        inputs, trainlabels = data\n",
    "        \n",
    "        #zero param gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #forward + backward + optimize\n",
    "        outputs = modelC(inputs)\n",
    "        loss = criterion(outputs, trainlabels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #print stats\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999: #print every 2000 mini-batches\n",
    "            print('[%d,%5d] loss: %.3f' % (epoch+1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Images Tested = 10000\n",
      "\n",
      "Model Accuracy = 0.6905\n"
     ]
    }
   ],
   "source": [
    "# To test modelC on the data\n",
    "# getting predictions on test set and measuring the performance\n",
    "correct_count, all_count = 0, 0\n",
    "for inp,labels in test_loader:\n",
    "  for i in range(len(labels)):\n",
    "    with torch.no_grad():\n",
    "        logps = modelC(inp)\n",
    "\n",
    "    \n",
    "    ps = torch.exp(logps)\n",
    "    probab = list(ps.numpy()[0])\n",
    "    pred_label = probab.index(max(probab))\n",
    "    true_label = labels.numpy()[i]\n",
    "    if(true_label == pred_label):\n",
    "      correct_count += 1\n",
    "    all_count += 1\n",
    "\n",
    "print(\"Number Of Images Tested =\", all_count)\n",
    "print(\"\\nModel Accuracy =\", (correct_count/all_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(modelC.state_dict(), '/home/brian_chen/modelC.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelC = ModelC()\n",
    "modelC.load_state_dict(torch.load('/home/brian_chen/modelC.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelD, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 8, kernel_size=3, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size = 3, padding = 1)\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size = 4, padding = 1)\n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size = 3, padding = 1)\n",
    "        self.conv6 = nn.Conv2d(128,256, kernel_size = 2)\n",
    "        self.fc1 = nn.Linear(6*6*256, 256)\n",
    "        self.fc2 = nn.Linear(256,128)\n",
    "        self.fc3 = nn.Linear(128,64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x),2))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(F.max_pool2d(self.conv4(x),2))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = x.view(-1,6*6*256)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "modelD = ModelD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(modelD.parameters(), lr = 0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2000] loss: 2.303\n",
      "[1, 4000] loss: 2.304\n",
      "[1, 6000] loss: 2.303\n",
      "[1, 8000] loss: 2.303\n",
      "[1,10000] loss: 2.303\n",
      "[2, 2000] loss: 2.303\n",
      "[2, 4000] loss: 2.303\n",
      "[2, 6000] loss: 2.303\n",
      "[2, 8000] loss: 2.303\n",
      "[2,10000] loss: 2.303\n",
      "[3, 2000] loss: 2.303\n",
      "[3, 4000] loss: 2.303\n",
      "[3, 6000] loss: 2.303\n",
      "[3, 8000] loss: 2.303\n",
      "[3,10000] loss: 2.303\n",
      "[4, 2000] loss: 2.303\n",
      "[4, 4000] loss: 2.303\n",
      "[4, 6000] loss: 2.303\n",
      "[4, 8000] loss: 2.303\n",
      "[4,10000] loss: 2.303\n",
      "[5, 2000] loss: 2.303\n",
      "[5, 4000] loss: 2.303\n",
      "[5, 6000] loss: 2.303\n",
      "[5, 8000] loss: 2.303\n",
      "[5,10000] loss: 2.303\n",
      "[6, 2000] loss: 2.303\n",
      "[6, 4000] loss: 2.303\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-4a4e65b1e7fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-406b9570512d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    417\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 419\u001b[0;31m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0m\u001b[1;32m    420\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(40): #loop over the data set \n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader,0):\n",
    "        #get inputs; data is list of [inputs,labels]\n",
    "        inputs, trainlabels = data\n",
    "        \n",
    "        #zero param gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #forward + backward + optimize\n",
    "        outputs = modelD(inputs)\n",
    "        loss = criterion(outputs, trainlabels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #print stats\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999: #print every 2000 mini-batches\n",
    "            print('[%d,%5d] loss: %.3f' % (epoch+1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test modelD on the data\n",
    "# getting predictions on test set and measuring the performance\n",
    "correct_count, all_count = 0, 0\n",
    "for inp,labels in validation_loader:\n",
    "  for i in range(len(labels)):\n",
    "    with torch.no_grad():\n",
    "        logps = modelD(inp)\n",
    "\n",
    "    \n",
    "    ps = torch.exp(logps)\n",
    "    probab = list(ps.numpy()[0])\n",
    "    pred_label = probab.index(max(probab))\n",
    "    true_label = labels.numpy()[i]\n",
    "    if(true_label == pred_label):\n",
    "      correct_count += 1\n",
    "    all_count += 1\n",
    "\n",
    "print(\"Number Of Images Tested =\", all_count)\n",
    "print(\"\\nModel Accuracy =\", (correct_count/all_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(modelD.state_dict(), '/home/brian_chen/modelD.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelD = ModelD()\n",
    "modelD.load_state_dict(torch.load('/home/brian_chen/modelD.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze these models\n",
    "for param in modelA.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "for param in modelB.parameters():\n",
    "    param.requires_grad_(False)\n",
    "    \n",
    "for param in modelC.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "for param in modelD.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEnsemble(nn.Module):\n",
    "    def __init__(self, modelA, modelB, modelC, modelD, nb_classes=10):\n",
    "        super(MyEnsemble, self).__init__()\n",
    "        self.modelA = modelA\n",
    "        self.modelB = modelB\n",
    "        self.modelC = modelC\n",
    "        self.modelD = modelD\n",
    "        # Remove last linear layer\n",
    "        self.modelA.fc = nn.Identity()\n",
    "        self.modelB.fc = nn.Identity()\n",
    "        self.modelC.fc = nn.Identity()\n",
    "        self.modelD.fc = nn.Identity()\n",
    "        \n",
    "        # Create new classifier\n",
    "        self.classifier = nn.Linear(256+256+256, nb_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.modelA(x.clone())  # clone to make sure x is not changed by inplace methods\n",
    "        x1 = x1.view(x1.size(0), -1)\n",
    "        x2 = self.modelB(x)\n",
    "        x2 = x2.view(x2.size(0), -1)\n",
    "        x3 = self.modelC(x)\n",
    "        x3 = x3.view(x3.size(0), -1)\n",
    "        x4 = self.modelD(x)\n",
    "        x4 = x4.view(x4.size(0), -1)\n",
    "        x = torch.cat((x1, x2, x3, x4), dim=1)\n",
    "        \n",
    "        x = self.classifier(F.relu(x))\n",
    "        return x\n",
    "    \n",
    "modelD = MyEnsemble(modelA, modelB, modelC, modelD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
